{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManoharRavula/Manohar_INFO5731_-Spring2024/blob/main/Ravulapalli_Manohar_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "My topic for Text classification is identifying spam vs. non-spam emails. For this an effective feature extraction process is required for the machine learning model. By using\n",
        "the below features it can build the model and make it accurate -\n",
        "1)Bag of Words (BoW): This technique quantifies the presence and frequency of words within the email text as certain words and phrases are more prevalent in spam emails.\n",
        "2)TF-IDF (Term Frequency-Inverse Document Frequency): This feature is useful for assessing the significance of a word within a document compared to a group of documents or corpus.\n",
        " This feature proves useful in identifying spam emails by devaluing used words across all emails and emphasizing the importance of words, to individual emails.\n",
        "3)Checking the Length of the Email: The size of an email whether, in terms of word count or characters can be an indicator. Spam emails can show length traits like being\n",
        " unusually brief or lengthy compared to other emails. By including length, we can identify these trends.\n",
        "4)Checking Presence of URLs: When there are URLs in an email it mostly indicates spam. Spam messages often contain links that\n",
        "  direct recipients to external sites for phishing attacks, advertisements, or other unsolicited content.\n",
        "  By Tracking the count of URLs within each email we can enhance the model's ability to recognize potential spam by identifying the links.\n",
        "5)Use of Capital Letters: Spam emails often capitalize words to grab the readers attention whether in headers, subject lines or the message itself aiming to highlight urgency or significance.\n",
        " so by evaluating the percentage of capitalized text,we can detect the spam.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c0659b-42ec-4665-c59a-067f0c354fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted successfully!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import re\n",
        "\n",
        "# Sample email texts\n",
        "emails = [\n",
        "    \"WIN BIG! Click here to claim your prize NOW! http://example.com\",\n",
        "    \"Dear user, your account has been updated. Please log in to confirm.\",\n",
        "    \"Congratulations! You've won a $500 Amazon gift card. Click to redeem: http://phishing.com\",\n",
        "    \"Hi, could you please review the attached file? Thanks, John.\",\n",
        "    \"LIMITED TIME OFFER! Get your free trial of our product today!\",\n",
        "     \"Urgent: Your account will be locked unless you update your information immediately at http://scam-link.com\",\n",
        "    \"Hello friend, I am the son of a deposed prince. I need your help to secure my inheritance. I offer you a reward.\",\n",
        "    \"Are you looking for cheap flights? Exclusive deals just for you: http://deals-on-flights.com\",\n",
        "    \"This is a reminder that your next appointment is scheduled for tomorrow at 3 PM. Please call to confirm.\",\n",
        "    \"Team meeting at 10 AM in the main conference room. Please prepare your monthly reports.\",\n",
        "    \"Your subscription has been cancelled successfully. If this was a mistake, please contact support.\",\n",
        "    \"Final notice: Your payment is overdue. Log in through the link to avoid service interruption: http://fake-payment.com\",\n",
        "    \"Discover the new features of our app with the latest update. Click here to learn more.\",\n",
        "]\n",
        "\n",
        "# Labels for the sample data (1 for spam, 0 for non-spam)\n",
        "labels = [1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0]\n",
        "\n",
        "# Feature 1: Bag of Words (BoW)\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_features = count_vectorizer.fit_transform(emails)\n",
        "\n",
        "# Feature 2: TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(emails)\n",
        "\n",
        "# Feature 3: Length of the email (in terms of word count)\n",
        "length_features = np.array([len(email.split()) for email in emails]).reshape(-1, 1)\n",
        "\n",
        "# Feature 4: Presence of URLs\n",
        "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "url_counts = np.array([len(re.findall(url_pattern, email)) for email in emails]).reshape(-1, 1)\n",
        "\n",
        "# Feature 5: Use of Capital Letters\n",
        "capital_letter_counts = np.array([sum(1 for char in email if char.isupper()) for email in emails]).reshape(-1, 1)\n",
        "capital_letter_percentage = np.array([count / len(email) for count, email in zip(capital_letter_counts, emails)]).reshape(-1, 1)\n",
        "\n",
        "# Combining all features into a single feature matrix\n",
        "from scipy.sparse import hstack\n",
        "features = hstack([bow_features, tfidf_features, length_features, url_counts, capital_letter_percentage])\n",
        "\n",
        "print(\"Features extracted successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f420c57e-dc61-489f-bf6e-5f8223a44e42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('please', 5.833333333333332),\n",
              " ('com', 4.2857142857142865),\n",
              " ('http', 4.2857142857142865),\n",
              " ('Presence of URLs', 4.2857142857142865),\n",
              " ('been', 2.333333333333333),\n",
              " ('confirm', 2.333333333333333),\n",
              " ('has', 2.333333333333333),\n",
              " ('this', 2.333333333333333),\n",
              " ('you', 2.099206349206349),\n",
              " ('deals', 1.7142857142857144)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "\n",
        "# features is my combined feature matrix and labels are my target labels\n",
        "\n",
        "# Applying the Chi-squared test\n",
        "chi_scores, p_values = chi2(features, labels)\n",
        "\n",
        "# Ranking features based on the chi-squared scores\n",
        "feature_names = (count_vectorizer.get_feature_names_out().tolist() +\n",
        "                 tfidf_vectorizer.get_feature_names_out().tolist() +\n",
        "                 [\"Length of email\", \"Presence of URLs\", \"Use of Capital Letters Percentage\"])\n",
        "\n",
        "# Combining feature names and their chi-squared scores\n",
        "feature_chi_scores = zip(feature_names, chi_scores)\n",
        "\n",
        "# Sorting the features by their chi-squared scores in descending order\n",
        "sorted_features = sorted(feature_chi_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Displaying the top features based on chi-squared scores\n",
        "sorted_features[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c15ca49-cb03-4d19-c4ca-d5fc0fd4201b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked texts based on similarity to the query: 'Want to know the latest promotional offers? Click here!'\n",
            "Text: \"Discover the new features of our app with the latest update. Click here to learn more.\" - Similarity Score: 0.8666878938674927\n",
            "Text: \"WIN BIG! Click here to claim your prize NOW! http://example.com\" - Similarity Score: 0.8127886652946472\n",
            "Text: \"Congratulations! You've won a $500 Amazon gift card. Click to redeem: http://phishing.com\" - Similarity Score: 0.7976548671722412\n",
            "Text: \"Urgent: Your account will be locked unless you update your information immediately at http://scam-link.com\" - Similarity Score: 0.7758417129516602\n",
            "Text: \"Dear user, your account has been updated. Please log in to confirm.\" - Similarity Score: 0.753309428691864\n",
            "Text: \"Are you looking for cheap flights? Exclusive deals just for you: http://deals-on-flights.com\" - Similarity Score: 0.747409462928772\n",
            "Text: \"Hi, could you please review the attached file? Thanks, John.\" - Similarity Score: 0.745686411857605\n",
            "Text: \"Your subscription has been cancelled successfully. If this was a mistake, please contact support.\" - Similarity Score: 0.7381293773651123\n",
            "Text: \"Final notice: Your payment is overdue. Log in through the link to avoid service interruption: http://fake-payment.com\" - Similarity Score: 0.7289672493934631\n",
            "Text: \"LIMITED TIME OFFER! Get your free trial of our product today!\" - Similarity Score: 0.6914722919464111\n",
            "Text: \"This is a reminder that your next appointment is scheduled for tomorrow at 3 PM. Please call to confirm.\" - Similarity Score: 0.6820365190505981\n",
            "Text: \"Team meeting at 10 AM in the main conference room. Please prepare your monthly reports.\" - Similarity Score: 0.6818138360977173\n",
            "Text: \"Hello friend, I am the son of a deposed prince. I need your help to secure my inheritance. I offer you a reward.\" - Similarity Score: 0.6347860097885132\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initializing the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function for encoding the text into BERT embeddings\n",
        "def encode_text_to_bert_embeddings(text):\n",
        "    # Tokenizing and encoding text with padding, truncation, and return PyTorch tensors\n",
        "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "    # Getting the model output without gradient calculation\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded_input)\n",
        "    # Use mean pooling for the token embeddings to get a single sentence embedding\n",
        "    embeddings = output.last_hidden_state\n",
        "    attention_mask = encoded_input['attention_mask']\n",
        "    mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(embeddings * mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "    mean_pooled = sum_embeddings / sum_mask\n",
        "    return mean_pooled.cpu().numpy()\n",
        "\n",
        "# Encoding all the texts to BERT embeddings\n",
        "encoded_texts = [encode_text_to_bert_embeddings(text) for text in emails]\n",
        "\n",
        "# Encoding the query\n",
        "query = \"Want to know the latest promotional offers? Click here!\"\n",
        "encoded_query = encode_text_to_bert_embeddings(query)\n",
        "\n",
        "# Calculating the cosine similarities\n",
        "similarities = [cosine_similarity(encoded_query, text_emb)[0][0] for text_emb in encoded_texts]\n",
        "\n",
        "# Ranking texts by similarity\n",
        "ranked_texts_indices = sorted(range(len(emails)), key=lambda i: similarities[i], reverse=True)\n",
        "\n",
        "print(\"Ranked texts based on similarity to the query: 'Want to know the latest promotional offers? Click here!'\")\n",
        "for index in ranked_texts_indices:\n",
        "    print(f\"Text: \\\"{emails[index]}\\\" - Similarity Score: {similarities[index]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Coming up with a problem and how natural language processing can help in solving the problem was great. This assignment required alot of effort to understand feature\n",
        "extraction process and from the extracted features using chi-squared to rank was really something great. Using these may be i can train a model like this expanding\n",
        "my second question  X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=42)\n",
        "training a Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(features_scaled) and release a spam detector\n",
        "model.\n",
        "Understanding questions and the output we need to get was the challenging part, modifying the code, analyzing required so much time.\n",
        "The exercise demonstrated several core NLP concepts and technique like Text Representation using Bert model, Information Retrieval using cosine similarity and\n",
        "how to combine features into single matrix for chi squared test. Greatful for this assigment.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}